# ğŸ” COMPAS Recidivism Bias Analysis using AI Fairness 360

Welcome to the **COMPAS Bias Analysis** project! This repository investigates racial bias in the COMPAS recidivism dataset using IBMâ€™s âš–ï¸ [AI Fairness 360 Toolkit (AIF360)](https://aif360.mybluemix.net/). The goal is to identify and mitigate algorithmic bias in criminal justice risk assessment systems.

---

## ğŸ“ Project Structure

-- AI Fairness.ipynb # Main analysis script
-- Fair AI Systems.pdf # Project report
-- README.md # You're here!
-- Compas-scores-two-years # Dependencies



---

## ğŸš€ Whatâ€™s Inside

### âœ… **Bias Detection**
- Uses the COMPAS dataset published by ProPublica.
- Filters dataset based on legal & ethical guidelines (e.g., removing low-level charges, incomplete data).
- Encodes categorical variables and defines `race_binary` attribute for fairness analysis.

### ğŸ“Š **Fairness Metrics**
- **Mean Difference**: Measures how favorable outcomes differ across racial groups.
- **Disparate Impact**: Evaluates if outcomes are proportionate between privileged (Caucasian) and unprivileged groups.

### ğŸ› ï¸ **Remediation Steps**
- Dataset reweighing ğŸ§®
- Fair classifier training ğŸ¤–
- Visualization of disparities ğŸ“‰

---

## ğŸ§  Key Findings

> ğŸ“‰ Disparate Impact ~ 0.76  
> âš ï¸ Mean Difference ~ -0.19  
These suggest measurable racial bias disadvantaging non-Caucasian individuals in risk predictions.

---

## ğŸ“Œ Requirements

```bash
!pip install aif360
!pip install pandas numpy scikit-learn matplotlib seaborn
```

### ğŸ™Œ Acknowledgements
ProPublica for the original dataset.

IBM Research for the AIF360 toolkit.

PLP for the course content

### âœ¨ Contribute
Want to help make AI fairer?
Feel free to fork, star â­, or open a pull request!


